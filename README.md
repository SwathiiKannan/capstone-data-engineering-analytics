# ğŸ Capstone Data Engineering & Analytics Project

An **end-to-end data engineering and analytics pipeline** built using Python, Apache Airflow, PostgreSQL and Power BI. The project covers everything from raw data ingestion to visualization.

---

## ğŸ” Full End-to-End Data Pipeline Flow

1ï¸âƒ£ Raw Excel File
â†“
2ï¸âƒ£ Clean using pandas (Python script)
â†“
3ï¸âƒ£ Load Clean Data to PostgreSQL (Data Warehouse)
â†“
4ï¸âƒ£ Schedule & Orchestrate using Apache Airflow
â†“
5ï¸âƒ£ Query using SQL (in PostgreSQL)
â†“
6ï¸âƒ£ Visualize in Power BI


## ğŸ¯ Project Objective

To build a complete data pipeline and dashboard using a real-world dataset.  
This includes:

- Data ingestion
- ETL processing using Python & pandas
- Workflow orchestration using Apache Airflow
- Data warehousing-ready structure
- SQL querying for analysis
- Dashboarding using Power BI

---

## ğŸ› ï¸ Tech Stack

- **Languages**: Python, SQL  
- **Tools & Libraries**:
  - `pandas`, `openpyxl` â€“ for data transformation
  - `Apache Airflow` â€“ for orchestration
  - `Power BI` â€“ for dashboards
- **IDE**: VS Code  
- **Version Control**: Git + GitHub  
- **Data Storage**: PostgreSQL (Data Warehouse), Local filesystem
- **Environment**: Python 3.12 + pip packages (`requirements.txt`)

---

## ğŸ“¦ Dataset Description

A global sales dataset containing over 51,000 transactions including customer, product, sales, and logistics data.

- **Source**: Kaggle â€“ [Global Superstore 2016](https://www.kaggle.com/datasets)
- **Format**: `.xlsx` Excel file
- **Size**: 51,290 Rows Ã— 24 Columns

### ğŸ”‘ Key Features

- `Order Date`, `Ship Date`: Order and shipping timeline
- `Customer ID`, `Segment`, `Region`, `Country`, etc.: Demographics
- `Product ID`, `Category`, `Sub-Category`: Product information
- `Sales`, `Discount`, `Profit`, `Shipping Cost`: Financial metrics
- `Ship Mode`, `Order Priority`: Order logistics

---

### ğŸ§  Use Cases

- Analyze **sales trends** globally
- Segment **customer behavior** by region
- Assess **profitability by product** and geography
- Evaluate **shipping efficiency** and delivery performance

---

### âš ï¸ Data Quality Notes

- `Postal Code` is 80% missing â†’ excluded from final model
- Other columns are well-structured with minimal cleanup needed
- Missing values handled with standard pandas operations (`dropna`, `fillna`)

---

## ğŸ—‚ï¸ Project Structure

## ğŸ§¹ Data Cleaning & Preprocessing (Python + pandas)

The raw dataset was cleaned and preprocessed using pandas in Python. Below are the key steps performed to prepare the data for loading into PostgreSQL.

### ğŸ”§ Cleaning Steps
- Loaded the dataset from Excel using pandas.read_excel().
- **Inspected data**:
  Checked shape, column types, null values. Viewed descriptive stats and sample rows
- **Dropped columns**:
  'Postal Code' was ~80% missing â†’ removed
- **Validated & retained date formats**:
  Order Date and Ship Date were correctly typed as datetime64
- **Rounded & converted numeric columns**:
  Sales, Profit, Shipping Cost, Discount â†’ rounded and converted to int
- **Saved cleaned data to a new Excel file**
  â¤ [`data/Data_Cleaned/global_superstore_20161_cleaned.xlsx`](data/Data_Cleaned/global_superstore_20161_cleaned.xlsx)


 





